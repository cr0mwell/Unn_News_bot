{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from scipy.sparse import hstack, vstack, save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "PROJ_PATH = r'F:\\tmp\\data science\\UnnParserBot'\n",
    "# CLASSES   = ['економіка', 'кримінал', 'культура', 'міжнародні новини', 'позитив', 'політика', 'спорт', 'суспільство', 'технології']\n",
    "CLASSES   = ['економіка', 'кримінал', 'міжнародні новини', 'політика', 'спорт', 'суспільство']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "morph = MorphAnalyzer(lang='uk')\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    for char in chain(string.punctuation, ['“', '”', '„', '–', '—', '…']):\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "def stemmed_text(text):\n",
    "    return ' '.join([morph.parse(tok)[0].normal_form for tok in text.split(' ')])\n",
    "\n",
    "STOP_WORDS = stemmed_text(' '.join(get_stop_words('ukrainian'))).split(' ')\n",
    "\n",
    "with open(os.path.join(PROJ_PATH, 'src', 'categorized_news.json'), encoding='utf-8') as f:\n",
    "    content       = json.load(f)\n",
    "    article_names = []\n",
    "    texts         = []\n",
    "    headings      = []\n",
    "    \n",
    "    for id in content.keys():\n",
    "        article_names.append(strip_punctuation(content[id]['name']))\n",
    "        texts.append(strip_punctuation(content[id]['text']))\n",
    "        # there are only few samples of the following headings in the train text block:\n",
    "        # 'культура', 'технології', 'позитив'\n",
    "        # therefore classifying those as 'суспільство' for now\n",
    "        if content[id]['category'] in ['культура', 'технології', 'позитив']:\n",
    "            headings.append('суспільство')\n",
    "        else:\n",
    "            headings.append(content[id]['category'])\n",
    "\n",
    "len(article_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: економіка\n",
      "1: кримінал\n",
      "2: міжнародні новини\n",
      "3: політика\n",
      "4: спорт\n",
      "5: суспільство\n"
     ]
    }
   ],
   "source": [
    "# Encoding the target\n",
    "le = LabelEncoder()\n",
    "le.fit(CLASSES)\n",
    "headings = le.transform(headings)\n",
    "\n",
    "for class_, val in zip(np.unique(headings), le.inverse_transform(np.unique(headings))):\n",
    "    print(f'{class_}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectorizer object\n",
    "tfidf_w = TfidfVectorizer(analyzer='word',        # token = word\n",
    "                          sublinear_tf=True,\n",
    "                          ngram_range=(1, 2),     # (1, 1) - only unigrams are used, (1,2) - unigrams/bigrams, etc.\n",
    "                          stop_words=STOP_WORDS,  # list of words to filter or None\n",
    "                          vocabulary=None,        # or dict - own_dictionary of words to process\n",
    "                          max_df=0.8,             # a frequency limit to filter the words by\n",
    "                          max_features=5000,      # only top N words will be used as columns,\n",
    "                          smooth_idf=True,    \n",
    "                          norm='l2'               # euclidean norm is used by default\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char vectorizer object\n",
    "tfidf_ch = TfidfVectorizer(analyzer='char',        # token = word\n",
    "                           ngram_range=(2, 6),     # (1, 1) - only unigrams are used, (1,2) - unigrams/bigrams, etc.\n",
    "                           vocabulary=None,        # or dict - own_dictionary of words to process\n",
    "                           max_df=0.8,             # a frequency limit to filter the words by\n",
    "                           max_features=15000,     # only top N words will be used as columns,\n",
    "                           smooth_idf=True,    \n",
    "                           norm='l2'               # euclidean norm is used by default\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the texts into train/test folds\n",
    "texts_train, texts_test, y_train, y_test = train_test_split([[text] for text in texts],\n",
    "                                                            headings,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=headings,\n",
    "                                                            random_state=42)\n",
    "\n",
    "# As texts_train, texts_test are now list of lists ([['foo'], ['bar']]), converting them to lists (['foo', 'bar'])\n",
    "texts_train = [t[0] for t in texts_train]\n",
    "texts_test  = [t[0] for t in texts_test]\n",
    "headings    = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['дев', 'ятий', 'ятнадцятий', 'ятнадцять', 'ять', 'ім'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((24800, 20000), (6200, 20000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Apply TfidfVectorizers to the texts and build the combined train/test matrices\n",
    "tfidf_w.fit(texts_train)\n",
    "tfidf_ch.fit(texts_train)\n",
    "\n",
    "# Collecting feature names\n",
    "tfidf_w_labels  = [k for k,v in sorted(list(tfidf_w.vocabulary_.items()), key=lambda x: x[1])]\n",
    "tfidf_ch_labels = [k for k,v in sorted(list(tfidf_ch.vocabulary_.items()), key=lambda x: x[1])]\n",
    "orig_features   = tfidf_w_labels + tfidf_ch_labels\n",
    "        \n",
    "w_train  = tfidf_w.transform(texts_train)\n",
    "ch_train = tfidf_ch.transform(texts_train)\n",
    "train_ds = hstack([w_train, ch_train])\n",
    "\n",
    "w_test  = tfidf_w.transform(texts_test)\n",
    "ch_test = tfidf_ch.transform(texts_test)\n",
    "test_ds = hstack([w_test, ch_test])\n",
    "\n",
    "train_ds.shape, test_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31000, 20000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging train and test\n",
    "all_ds = vstack([train_ds, test_ds])\n",
    "all_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we need to show the generated data...\n",
    "#data['article_names'] = article_names\n",
    "#labels_w = [k for k,v in sorted(list(tfidf_w.vocabulary_.items()), key=lambda x: x[1])]\n",
    "#labels_ch = [k for k,v in sorted(list(tfidf_ch.vocabulary_.items()), key=lambda x: x[1])]\n",
    "#data.update(dict(zip(labels_w + labels_ch, texts_transformed.T.toarray())))\n",
    "#data.update({'headings': headings})\n",
    "#dataset = pd.DataFrame(data)\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the all_ds and headings\n",
    "#save_npz(os.path.join(PROJ_PATH, 'src', 'class_data.npz'), all_ds)\n",
    "#with open(os.path.join(PROJ_PATH, 'src', 'class_headings.hd'), 'wb') as f:\n",
    "#    pickle.dump(headings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectorizer objects\n",
    "#with open(os.path.join(PROJ_PATH, 'src', 'class_tfidf_w.vct'), 'wb') as f:\n",
    "#    pickle.dump(tfidf_w, f)\n",
    "#    \n",
    "#with open(os.path.join(PROJ_PATH, 'src', 'class_tfidf_ch.vct'), 'wb') as f:\n",
    "#    pickle.dump(tfidf_ch, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
